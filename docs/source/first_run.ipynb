{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MindAffect BCI: 1st Run Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the mindaffectBCI decoder and other required modules. \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%gui qt\n",
    "import mindaffectBCI.decoder\n",
    "from multiprocessing import Process\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "System Architecture\n",
    "--\n",
    "The basic architecture of a mindaffectBCI is illustrated here.\n",
    "![SystemArchitecture.png](images/SystemArchitecture.png)\n",
    "\n",
    "To actually run the BCI we need to start each of these components:\n",
    "- UtopiaHub: This component is the central server which coordinates all the other pieces, and saves the data for offline analysis\n",
    "\n",
    "- Acquisition: This component talks to the *EEG Headset* and streams the data to the Hub\n",
    "\n",
    "- Decoder: This component analysis the EEG data to fit the subject specific model and generate predictions\n",
    "\n",
    "- Presentation: This component presents the User-Interface to the user, including any BCI specific stimuli which need to be presented. It also selects outputs when the BCI is sufficiently confident and generates the appropriate output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we start the UtopiaHub by running the following code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------- HUB ------------------------------\n",
    "# start the utopia-hub process\n",
    "from mindaffectBCI.decoder import startUtopiaHub\n",
    "hub = Process(target=startUtopiaHub.run, daemon=True)\n",
    "hub.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACQUISITION "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the hub is running we want to establish a connection between the amplifier and the hub to stream the EEG data.\n",
    "To achieve this the [Brainflow](https://brainflow.ai/) library is used. The brainflow driver has to be initialized with input parameters that depend on the amplifier in use: (Consult the [Brainflow docs](https://brainflow.readthedocs.io/en/stable/SupportedBoards.html) for a complete list of amplifiers supported by brainflow but currently untested with the MindAffect BCI.)\n",
    "\n",
    "|**Board** | ```board_id```|```serial_port```|```ip_address```|```ip_port```| \n",
    "|:-----------|:---------------|:-----------------|:----------------|:-------------|  \n",
    "|Ganglion  | 1             |dongle serial port(COM3, /dev/ttyUSB0...)| - | - |\n",
    "|Ganglion + WiFi Shield|4 |          -            |WIFI Shield IP(default 192.168.4.1)|any local port which is free|\n",
    "|Cyton     | 0             |dongle serial port(COM3, /dev/ttyUSB0...)| -| -|\n",
    "|Cyton + Wifi Shield|5|-|WIFI Shield IP(default 192.168.4.1)|any local port which is free|\n",
    "\n",
    "When using either the OpenBCI Ganglion or Cyton with an USB-dongle we have to pass the ```serial_port``` argument, to find the serial port in use by your amplifier follow the following instructions: \n",
    "\n",
    "### On Mac:\n",
    "1. Open a Terminal session\n",
    "2. Type: `ls /dev/cu.*`, and look for something like `/dev/cu.usbmodem1` (or similar):\n",
    "\n",
    "    ```\n",
    "    $ ls /dev/cu.*\n",
    "    /dev/cu.Bluetooth-Modem\t\t/dev/cu.iPhone-WirelessiAP\n",
    "    /dev/cu.Bluetooth-PDA-Sync\t/dev/cu.usbserial\n",
    "    /dev/cu.usbmodem1\n",
    "    ```\n",
    "\n",
    "    Then, `serial_port` should be defined as  `\"serial_port\":\"dev/cu.your_com_name\"`\n",
    "\n",
    "### On Windows:\n",
    "\n",
    "1. Open Device Manager and unfold Ports(COM&LPT), the com port number is shown behind your used bluetooth adapter. \n",
    "    ![comport.jpg](images/comport.jpg)\n",
    "    Then, `serial_port` should be defined as  `\"serial_port\":\"COM_X_\"`\n",
    "\n",
    "The code below shows the `acq_args` for the Ganglion, change the arguments for the board in use and run the code block to start the acquisition process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the ganglion acquisition process\n",
    "# Using brainflow for the acquisition driver.  \n",
    "#  the brainflowargs are kwargs passed to BrainFlowInputParams\n",
    "#  so change the board_id and other args to use other boards\n",
    "from mindaffectBCI.examples.acquisition import utopia_brainflow\n",
    "acq_args =dict(board_id=1, serial_port='com4') # connect to the ganglion\n",
    "acquisition = Process(target=utopia_brainflow.run, kwargs=acq_args, daemon=True)\n",
    "acquisition.start()\n",
    "# wait for driver to startup -- N.B. NEEDED!!\n",
    "sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N.B. Only use this cell if you just want to test with a *fake* eeg stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start a fake-data stream\n",
    "# with 4-channels running at 200Hz\n",
    "from mindaffectBCI.examples.acquisition import utopia_fakedata\n",
    "acq_args=dict(host='localhost', nch=4, fs=200)\n",
    "acquisition = Process(target=utopia_fakedata.run, kwargs=acq_args, daemon=True)\n",
    "acquisition.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DECODER "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder is the core of the BCI as it takes in the raw EEG and stimulus information and generates predictions about which stimulus the user is attending to. Generating these predictions relies on signal processing and machine learning techniques to learn the best decoding parameters for each user. However, ensuring best performance means the settings for the decoder should be appropriate for the particular BCI being used. The default decoder parameters are shown in the code below, and are setup for a noisetagging BCI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the decoder process, wih default args for noise-tagging\n",
    "from mindaffectBCI.decoder import decoder\n",
    "decoder = Process(target=decoder.mainloop, kwargs=dict(\n",
    "        stopband=[[45,65],[0,3],[25,-1]],\n",
    "        out_fs=100,\n",
    "        evtlabs=[\"re\",\"fe\"],\n",
    "        tau_ms=450,\n",
    "        calplots=True,\n",
    "        predplots=False\n",
    "    ), daemon=True)\n",
    "decoder.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key parameters here are:\n",
    "\n",
    "- `stopband`: this is a [temporal filter](https://en.wikipedia.org/wiki/Filter_(signal_processing)) which is applied as a pre-processing step to the incoming  data.  This is important to remove external noise so the decoder can focus on the target brain signals.   Here the filter is specified as a list of [band stop](https://en.wikipedia.org/wiki/Band-stop_filter) filters, which specify which signal frequencies should be suppressed, (where, in classic python fashion -1 indicates the max-possible frequency).  Thus, in this example all frequencies below 3Hz and above 25Hz are removed.\n",
    "\n",
    "- `out_fs`: this specifies the post-filtering sampling rate of the data.  This reduces the amount of data which will be processed by the rest of the decoder.  Thus, in this example after filtering the data is re-sampled to 80Hz.  (Note: to avoid []() out_fs should be greater than 2x the maximum frequency passed by the stop-band).\n",
    "\n",
    "- `evtlabs`: this specifies the stimulus properties (or event labels) the decoder will try to predict from the brain responses.  The input to the decoder (and the brain) is the raw-stimulus intensity (i.e. it's brightness, or loudness).  However, depending on the task the user is performing, the brain may *not* respond directly to the brightness, but some other property of the stimulus.  For example, in the classic [P300 'odd-ball' BCI](https://en.wikipedia.org/wiki/P300_(neuroscience)#Applications), the brain responds not to the raw intensity, but to the start of *surprising* stimuli.  The design of the P300 matrix-speller BCI means this response happens when the user's chosen output 'flashes', or gets bright.  Thus, in the P300 BCI the brain responds to the [rising-edge](https://en.wikipedia.org/wiki/Signal_edge) of the stimulus intensity.   Knowing exactly what stimulus property the brain is responding to is a well studied neuroscientific research question, with examples including, stimulus-onset (a.k.a. rising-edge, or 're'), stimulus-offset (a.k.a. falling-edge, or 'fe'), stimulus intensity ('flash'), stimulus-duration etc.  Getting the right stimulus-coding is critical for BCI performance, see [`stim2event.py`](mindaffectBCI/decoder/stim2event.py) for more information on supported event types.\n",
    "\n",
    "- `tau_ms`: this specifies the maximum duration of the expected brain response to a triggering event in *milliseconds*.  As with the trigger type, the length of the brian response to a triggering event depends on the type of response expected.  For example for the P300 the response is between 300 and 600 ms after the trigger, whereas for a VEP the response is between 100 and 400 ms.   Ideally, the response window should be as small as possible, so the learning system only gets the brain response, and not a lot of non-response containing noise which could lead the machine learning component to [overfitt](https://en.wikipedia.org/wiki/Overfitting).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRESENTATION \n",
    "\n",
    "Before launching the presentation component we first make sure that the Hub, Acquisition, and Decoder components are running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hub running False\n",
      "Acquisition running True\n",
      "Decoder running True\n"
     ]
    }
   ],
   "source": [
    "# check all is running?\n",
    "print(\"Hub running {}\".format(hub.is_alive()))\n",
    "print(\"Acquisition running {}\".format(acquisition.is_alive()))\n",
    "print(\"Decoder running {}\".format(decoder.is_alive()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If not, try running the corresponding codeblock of the inactive component before continuing.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Stimulus  \n",
    "By default we use the mindaffect NoiseTagging style stimulus with a 25-symbol letter matrix for presentation.  You can easily try different types of stimulus and selection matrices by modifying the `symbols` and `stimfile`arguments. Where:\n",
    "* _symbols_ : can either by a list-of-lists of the actual text to show, for example for a 2x2 grid of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " symbols=[[\"I'm happy\",\"I'm sad\"], [\"I want to play\",\"I want to sleep\"]],"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or a file from which to load the set of symbols as a *comma-separated* list of strings like the file [symbols.txt](https://github.com/mindaffect/pymindaffectBCI/blob/open_source/mindaffectBCI/examples/presentation/symbols.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols=\"symbols.txt\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* _stimfile_ : is a file which contains the stimulus-code to display.  This can either be a text-file with a matrix specified with a white-space separated line per output or a png with the stimulus with outputs in 'x' and time in 'y'. This is what the _codebook_ for the noisetag looks like where symbols are left to right and time is top to bottom.\n",
    "<img src=\"mgold_61_6521_psk_60hz.png\" width=\"200\"> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Stimulus\n",
    "The UI with the desired presentation stimuli can be launched by running the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the presentation, with our matrix and default parameters for a noise tag\n",
    "from mindaffectBCI.examples.presentation import selectionMatrix\n",
    "selectionMatrix.run(symbols=symbols, stimfile=\"mgold_65_6532_psk_60hz.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the system is up and running, you can go through the following steps to use the BCI!\n",
    "\n",
    "1. EEG headset Setup\n",
    "\n",
    "    Prepare a headset such that it follows the [MindAffect headset layout.pdf](https://github.com/mindaffect/Headset/blob/master/MindAffect%20headset%20layout.pdf) in our Headset repository or prepare the headset delivered with your kit by following [MindAffect headset setup.pdf](https://github.com/mindaffect/Headset/raw/master/MindAffect%20Headset%20Set%20up%20instructions.pdf)\n",
    "\n",
    "\n",
    "2. Signal Quality\n",
    "\n",
    "    Check the signal quality by pressing 0 in the main menu. Try to adjust the headset until all electrodes are green, or noise to signal ratio is below 5. \n",
    "    You can try to improve the signal for an electrode by pressing it firmly into your head. After releasing pressure, wait a few seconds to see if the signal improves. If not, remove the electrode, and apply more water to the sponge. The sponges should feel wet on your scalp.\n",
    "    If  the noise to signal ratio does not improve by adjusting the headset, try to distance yourself from power outlets and other electronics.\n",
    "\n",
    "\n",
    "3. Calibration\n",
    "\n",
    "    Start calibration by pressing 1 in the main menu. Continue to follow the on-screen instructions.\n",
    "\n",
    "\n",
    "4. Feedback\n",
    "\n",
    "    You are now ready to try out the BCI by either selecting Copy-spelling (2) or Free-spelling (1)!\n",
    "    \n",
    "**Struggeling to get the system to work? Consult our** [FAQ](https://mindaffect-bci.readthedocs.io/en/latest/FAQ.html) **section for info on how to improve calibration accuracy, prediction performance, and more!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do you not want to run this notebook everytime when using the BCI?\n",
    "Simply run it from your command prompt:    \n",
    "  \n",
    "``` python3 -m mindaffectBCI.online_bci```  \n",
    "   \n",
    "Or with one of the 3 provided configuration files:  \n",
    "  \n",
    "``` python3 -m mindaffectBCI.online_bci --config_file noisetag_bci.json ```  \n",
    "\n",
    " * [noisetag.json](https://github.com/mindaffect/pymindaffectBCI/tree/open_source/mindaffectBCI/noisetag_bci.json) : example for a [noise-tagging](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0133797) (or c-VEP) BCI (Default)\n",
    " * [rc5x5.json](https://github.com/mindaffect/pymindaffectBCI/tree/open_source/mindaffectBCI/rc5x5_bci.json) : example for a classic visual P300 odd-ball type BCI with row-column stimulus.\n",
    " * [ssvep.json](https://github.com/mindaffect/pymindaffectBCI/tree/open_source/mindaffectBCI/ssvep_bci.json) : example for a classic [steady-state-visual-response](https://arxiv.org/abs/2002.01171) BCI.\n",
    " \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHUTDOWN \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# shutdown the background processes\n",
    "decoder.terminate()\n",
    "hub.terminate()\n",
    "acquisition.terminate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
